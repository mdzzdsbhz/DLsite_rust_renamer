// src/scraper/scraper.rs

use std::collections::HashMap;
use thiserror::Error;
use std::path::Path;
use std::fs;
use ureq::Error; // Needed for Error
use ureq::Agent;
use std::time::Duration;
use std::io::Read;
use headless_chrome::Browser;

#[derive(Debug, Error)]
pub enum ScraperError {
    #[error("HTTP è¯·æ±‚å¤±è´¥: {0}")]
    HttpRequestError(String),
    #[error("ä½œå“ä¸å­˜åœ¨")]
    NotFound,
    #[error("é¡µé¢è§£æå¤±è´¥: {0}")]
    ParseError(String),
}

/// çˆ¬è™« Traitï¼šå¯å®ç°ä¸ºå„ç§ç«™ç‚¹çš„çˆ¬è™«
pub trait Scraper {
    fn fetch_page(&self, url: &str) -> Result<String, ScraperError>;
    /// ä¸‹è½½å°é¢å›¾åƒï¼ˆjpgï¼‰ï¼Œä¿å­˜ä¸º {ç›®æ ‡ç›®å½•}/cover.jpg
    fn download_cover<'a>(&'a self, rjcode: &'a str, dest_dir: &'a std::path::Path) -> std::pin::Pin<Box<dyn std::future::Future<Output = Option<()>> + Send + 'a>>;
    fn fetch_page_json(&self, url: &str) -> Result<serde_json::Value, ScraperError>;
}

/// é»˜è®¤å®ç°ï¼Œç”¨äºæŠ“å–DLsiteé¡µé¢
pub struct DlsiteScraper {
    headers: HashMap<String, String>,
}

impl DlsiteScraper {
    pub fn new() -> Self {
        let mut headers = HashMap::new();
        headers.insert("User-Agent".into(), "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114 Safari/537.36".into());
        headers.insert("Accept-Language".into(), "ja-JP".into());

        DlsiteScraper { headers }
    }
}

impl Scraper for DlsiteScraper {
    fn fetch_page(&self, url: &str) -> Result<String, ScraperError> {
        println!("ğŸŒ å¼€å§‹è¯·æ±‚é¡µé¢: {}", url); // ğŸ§ª 1. çœ‹æ˜¯å¦è°ƒç”¨äº†

        // åˆ›å»º Agentï¼ˆå¦‚éœ€ä»£ç†ï¼Œå¯ç»§ç»­æ·»åŠ é…ç½®ï¼‰
        let agent = Agent::new_with_defaults();

        // æ„å»ºè¯·æ±‚å¹¶é™„åŠ è‡ªå®šä¹‰ headers
        let mut request = agent.get(url);
        for (key, value) in &self.headers {
            request = request.header(key, value);
        }

        // ğŸ§ª 2. å°è¯•å‘é€è¯·æ±‚
        println!("ğŸš€ å‘é€è¯·æ±‚ä¸­...");
        let mut response = match request.call() {
            Ok(resp) => {
                println!("âœ… æ”¶åˆ°å“åº”: {} {}", resp.status(), resp.status());
                resp
            }
            Err(ureq::Error::StatusCode(code)) => {
                println!("âŒ çŠ¶æ€ç é”™è¯¯: {}", code);
                if code == 404 || code == 410 {
                    return Err(ScraperError::NotFound);
                }
                return Err(ScraperError::HttpRequestError(format!("çŠ¶æ€ç é”™è¯¯: {}", code)));
            }
            Err(e) => {
                println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
                return Err(ScraperError::HttpRequestError(format!("è¯·æ±‚å¤±è´¥: {}", e)));
            }
        };

        // ğŸ§ª 3. å°è¯•è¯»å–å“åº”å†…å®¹
        println!("ğŸ“– æ­£åœ¨è¯»å–å“åº”å†…å®¹...");
        let body = String::new();
        if let Err(e) = response.body_mut().read_to_string() {
            println!("âŒ è¯»å–å“åº”å¤±è´¥: {}", e);
            return Err(ScraperError::HttpRequestError(format!("è¯»å–å“åº”å¤±è´¥: {}", e)));
        }

        println!("ğŸ“„ é¡µé¢å¤§å°: {} å­—èŠ‚", body.len());

        if body.contains("ã“ã®ä½œå“ã¯å­˜åœ¨ã—ã¾ã›ã‚“") {
            println!("âš ï¸ é¡µé¢æ˜¾ç¤ºä½œå“ä¸å­˜åœ¨");
            return Err(ScraperError::NotFound);
        }
        println!("body: {}", body);
        Ok(body)
    }




    fn fetch_page_json(&self, url: &str) -> Result<serde_json::Value, ScraperError> {
        println!("ğŸŒ å¼€å§‹è¯·æ±‚é¡µé¢: {}", url);

        let agent = Agent::new_with_defaults();
        let mut request = agent.get(url);
        for (key, value) in &self.headers {
            request = request.header(key, value);
        }

        println!("ğŸš€ å‘é€è¯·æ±‚ä¸­...");
        let mut response = match request.call() {
            Ok(resp) => {
                println!("âœ… æ”¶åˆ°å“åº”: {}", resp.status());
                resp
            }
            Err(ureq::Error::StatusCode(code)) => {
                println!("âŒ çŠ¶æ€ç é”™è¯¯: {}", code);
                if code == 404 || code == 410 {
                    return Err(ScraperError::NotFound);
                }
                return Err(ScraperError::HttpRequestError(format!("çŠ¶æ€ç é”™è¯¯: {}", code)));
            }
            Err(e) => {
                println!("âŒ è¯·æ±‚å¤±è´¥: {}", e);
                return Err(ScraperError::HttpRequestError(format!("è¯·æ±‚å¤±è´¥: {}", e)));
            }
        };

        println!("ğŸ“– æ­£åœ¨è¯»å–å“åº”å†…å®¹...");
        let mut body = String::new();
        let reader = response.body_mut(); // è·å–å­—èŠ‚æµ
        println!("reader = {:?}", reader);
        if let Err(e) = reader.read_to_string() {
            println!("âŒ è¯»å–å“åº”å¤±è´¥: {}", e);
            return Err(ScraperError::HttpRequestError(format!("è¯»å–å“åº”å¤±è´¥: {}", e)));
        }

        println!("ğŸ“„ å“åº”å¤§å°: {} å­—èŠ‚", body.len());

        // ğŸš¨ åªå–ç¬¬ä¸€è¡Œä½œä¸º JSON
        let first_line = body.lines().next().unwrap_or("");

        let json: serde_json::Value = match serde_json::from_str(first_line) {
            Ok(val) => val,
            Err(e) => {
                println!("âŒ è§£æ JSON å¤±è´¥: {}", e);
                return Err(ScraperError::HttpRequestError(format!("è§£æ JSON å¤±è´¥: {}", e)));
            }
        };

        // å¦‚æœæ˜¯æ•°ç»„ï¼Œåˆ™å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        if let Some(arr) = json.as_array() {
            if let Some(first) = arr.first() {
                return Ok(first.clone());
            } else {
                return Err(ScraperError::HttpRequestError("è¿”å›çš„ JSON æ•°ç»„ä¸ºç©º".to_string()));
            }
        } else {
            return Err(ScraperError::HttpRequestError("è¿”å›çš„ JSON ä¸æ˜¯æ•°ç»„".to_string()));
        }
    }




        /// ä¸‹è½½å°é¢å›¾åƒï¼ˆjpgï¼‰ï¼Œä¿å­˜ä¸º {ç›®æ ‡ç›®å½•}/cover.jpg
    fn download_cover<'a>(&'a self, rjcode: &'a str, dest_dir: &'a Path) -> std::pin::Pin<Box<dyn std::future::Future<Output = Option<()>> + Send + 'a>> {
        Box::pin(async move {
            let url = format!("https://img.dlsite.jp/modpub/images2/work/doujin/RJ{}/{}_img_main.jpg",
                &rjcode[2..5], rjcode);

            // You may need to use an async HTTP client here, e.g., reqwest, and ensure self.client is defined.
            // let img_bytes = self.client.get(&url).send().await.ok()?.bytes().await.ok()?;
            // For now, this is a placeholder for the actual download logic.
            // Replace the following lines with your actual async HTTP client logic.
            let img_bytes = match reqwest::get(&url).await.ok()?.bytes().await.ok() {
                Some(bytes) => bytes,
                None => return None,
            };
            let save_path = dest_dir.join("cover.jpg");
            fs::write(save_path, &img_bytes).ok()?;
            Some(())
        })
    }

}
